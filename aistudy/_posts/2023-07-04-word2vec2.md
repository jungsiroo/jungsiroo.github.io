---
layout: post
title: ë…¼ë¬¸ ë¦¬ë·° - Word2vec (2)
description: >
    Distributed Representations of words and phrases and their compositionality
hide_description: false
category: aistudy
image:
  path: https://github.com/jungsiroo/jungsiroo.github.io/assets/54366260/8313b01b-9991-4588-9968-28269e12233b
---

**í•´ë‹¹ ì¸ë„¤ì¼ì€ `Wonkook Lee` ë‹˜ì´ ë§Œë“œì‹  [`Thumbnail-Maker`](https://wonkooklee.github.io/thumbnail_maker/){:target="_blank"} ë¥¼ ì´ìš©í•˜ì˜€ìŠµë‹ˆë‹¤**
{:.figcaption}

í•´ë‹¹ ë…¼ë¬¸ì€ word2vec ì˜ í›„ì† ë…¼ë¬¸ìœ¼ë¡œì¨ vector í’ˆì§ˆê³¼ í•™ìŠµ ì†ë„ë¥¼ ë†’ì¸ ë°©ë²•ì— ëŒ€í•´ ì†Œê°œí•˜ê³  ìˆë‹¤. ì†Œê°œëŠ” ê°„ë‹¨íˆ ì—¬ê¸°ì„œ ë§ˆë¬´ë¦¬í•˜ê³  
ë…¼ë¬¸ì„ ë¦¬ë·°í•´ ë³´ë„ë¡ í•˜ì.

* this unordered seed list will be replaced by the toc
{:toc}

# ğŸŒŸ Abstract

í•´ë‹¹ ë…¼ë¬¸ ì´ì „ì— ë°œí‘œí•œ ë…¼ë¬¸ì—ì„œëŠ” `continuous Skip-gram` ëª¨ë¸ì„ í†µí•´ ê³ í’ˆì§ˆì˜ distributed vector representations ì„ í•™ìŠµí•˜ëŠ”
ë°©ë²•ì— ëŒ€í•´ ì„¤ëª…í–ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ë²¡í„°ì˜ í€„ë¦¬í‹°ì™€ í•™ìŠµ ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì†Œê°œí•œë‹¤.

* Subsampling of the frequent words
* Instead of Hierarchical Softmax, use `Negative Sampling`

ë˜í•œ ê´€ìš©ì–´ì ìœ¼ë¡œ ë§ì§€ ì•ŠëŠ” ë‹¨ì–´ë“¤ì˜ ì¡°í•©ë„ í‘œí˜„í•  ìˆ˜ ìˆëŠ” `phrase vector` ë¥¼ ì†Œê°œí•œë‹¤. (Air Canada ì²˜ëŸ¼ ì„œë¡œ ì—°ê´€ ì—†ëŠ” ë‹¨ì–´ë“¤ì´ ë§Œë‚œ ë‹¨ì–´ë“¤)

# ğŸŸï¸ Introduction

ì´ì „ ë…¼ë¬¸ì—ì„œ ì†Œê°œí•œ Skip-gram ëª¨ë¸ê°™ì€ ê²½ìš° êµ‰ì¥íˆ ë§ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ í†µí•´ ê³ í’ˆì§ˆì˜ words' representation vector ë¥¼ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆì—ˆë‹¤ê³  í•œë‹¤.
ì´ëŸ¬í•œ í•™ìŠµì´ ê°€ëŠ¥í–ˆë˜ ì´ìœ ëŠ” ì—¬íƒ€ ë‹¤ë¥¸ NNLM ëª¨ë¸ë“¤ê³¼ ë‹¤ë¥´ê²Œ dense matrix multiplications ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì´ë¼ê³  ë°íˆê³  ìˆë‹¤.

ì´ ë…¼ë¬¸ì—ì„œëŠ” ì´ Skip-gram ì˜ ì•„ë˜ì™€ ê°™ì€ ê°œì„ ì ì„ í†µí•´ ë” ë¹ ë¥´ê³  ì •í™•í•´ì§„ ëª¨ë¸ì„ ë³´ì—¬ì£¼ê³ ì í•œë‹¤.

> * Subsampling of frequent words
>   * ì´ ë°©ë²•ì„ í†µí•´ ì•½ 2~10ë°° ì†ë„ í–¥ìƒê³¼ ë¹ˆë„ íšŸìˆ˜ê°€ ë‚®ì€ ë‹¨ì–´ì— ëŒ€í•œ ì •í™•ë„ í–¥ìƒì„ ì´ë£¸
> * Noise Contrastive Estimation(NCE) / Negative Sampling
>   * ì´ ë°©ë²•ì„ í†µí•´ ì†ë„ í–¥ìƒ ë° ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì— ëŒ€í•´ ë” ì¢‹ì€ í’ˆì§ˆì˜ vector ìƒì„± ê°€ëŠ¥
> * Phrase Vector í•™ìŠµ
>   * ìì—°ìŠ¤ëŸ½ì§€ ëª»í•œ ë‹¨ì–´ ì¡°í•©ë“¤ì˜ ëŒ€í•œ vector í•™ìŠµì— í•œê³„ê°€ ìˆìŒ
>   * ë”°ë¼ì„œ phrase ì˜ representation ì„ ë‹´ì€ vector ë¥¼ í™œìš©í•´ Skip-gram ëª¨ë¸ì´ ì¡°ê¸ˆ ë” í’ë¶€í•´ì§ˆ ìˆ˜ ìˆì—ˆìŒ
>   * ex) vec("Montreal Canadiens") - vec("Montreal") + vec("Toronto") = vec("Torontor Maple Leafs")
{:.lead}

ê·¸ë ‡ê²Œ ì´ ë°©ë²•ì„ í†µí•´ ê°„ë‹¨í•œ vector addition ì—ì„œ ìœ ì˜ë¯¸í•œ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ì˜ˆë¥¼ ë“¤ë©´, vec("Russia") + vec("river") ëŠ”
vector("Volga River") ì™€ ê°™ì´ í‘œí˜„ì´ ëœë‹¤ê³  í•œë‹¤. ì´ëŸ¬í•œ í•©ì„±ì„±ì€ ë‹¹ì—°í•˜ì§€ ì•Šì€ ì–¸ì–´ì˜ ì´í•´ê°€ word vector representation ì˜ ê°„ë‹¨í•œ ê³„ì‚°ìœ¼ë¡œ ë³´ì¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.

# ğŸ•¹ï¸ Skip-gram Model

> Objective Function : ì¤‘ì‹¬ ë‹¨ì–´ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì£¼ë³€ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  í™•ë¥ ì˜ logê°’ maximize

$$
\frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p\left(w_{t+j} \mid w_{t}\right)
$$

ì¤‘ì‹¬ ë‹¨ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ c ê°œì˜ ë‹¨ì–´ë¥¼ ì‚´í´ë´„
{:.figcaption}

ê·¸ë ‡ë‹¤ë©´ í•´ë‹¹ log ê°’ì€ ì–´ë–»ê²Œ ì •ì˜ë ê¹Œ?

$$
p\left(w_{O} \mid w_{I}\right)=\frac{\exp \left(v_{w_{O}}^{\prime}{ }^{\top} v_{w_{I}}\right)}{\sum_{w=1}^{W} \exp \left(v_{w}^{\prime}{ }^{\top} v_{w_{I}}\right)}
$$

Skip-gram model with softmax function
{:.figcaption}

I ë²ˆì§¸ ë‹¨ì–´ì— ëŒ€í•œ ë²¡í„°ì™€ì˜ ë‚´ì  ì—°ì‚°ê°’ì€ exp ê°’ì´ ëª¨ë“  ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” ë‚´ì ê°’ì˜ exp í•©ìœ¼ë¡œ ë‚˜ëˆˆ ê²ƒìœ¼ë¡œ ê³„ì‚°í•œë‹¤.

ìœ„ ì‹ì—ì„œ ëˆˆì—¬ê²¨ ë´ì•¼í•  ì ì€ $$W$$ ì´ë‹¤. $$W$$ ëŠ” vocab ì˜ ê°œìˆ˜ì¸ë° ì´ ì‹ì´ ì‹¤ìš©ì ì´ì§€ ëª»í•œ ì´ìœ ëŠ” softmax ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ì´ $$W$$ ì— ë¹„ë¡€í•˜ê¸° ë•Œë¬¸ì´ë‹¤.
ë³´í†µ $$W$$ ëŠ” $$10^{5}-10^{7}$$ ì •ë„ ëœë‹¤ê³  í•œë‹¤.

## âœ‚ï¸ Hierarchical Softmax

ì´ì „ ë…¼ë¬¸ì—ì„œëŠ” ì—°ì‚°ëŸ‰ì´ ë§ì€ softmax ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¨ì–´ë¥¼ Huffman Binary Tree ë¥¼ ì´ìš©í•´ êµ¬ì„±í•˜ê³  ê·¸ì— ë”°ë¼ `Hierarchical Softmax` ë¥¼ ì´ìš©í•˜ì˜€ë‹¤.
ê·¸ì— ë”°ë¼ ê¸°ì¡´ $$O(W)$$ ë§Œí¼ ê±¸ë¦¬ë˜ time complexity ë¥¼ $$O(log_2(W))$$ ë§Œí¼ ì¤„ì¼ ìˆ˜ ìˆì—ˆë‹¤. 

í•˜ì§€ë§Œ ê·¸ ë°©ë²•ì— ëŒ€í•´ ì •í™•íˆ ì œì‹œë˜ì§€ ì•Šì•˜ëŠ”ë° ì´ë²ˆ ë…¼ë¬¸ì„ í†µí•´ ì•Œì•„ë³´ë„ë¡ í•˜ì.

![image](https://github.com/jungsiroo/jungsiroo.github.io/assets/54366260/f13a490c-2c0d-4783-be65-f48eda9dc27f)

Hierachical Softmax Example
{:.figcaption}

ë¨¼ì € ì•Œê³  ê°€ì•¼í•˜ëŠ” ì ë“¤ì´ ìˆë‹¤. ë‚˜ëŠ” ë…¼ë¬¸ì„ ì½ì„ ë•Œ ì´ ë¶€ë¶„ì—ì„œ ì˜¤ëœ ì‹œê°„ ê±¸ë ¸ì—ˆë‹¤.

* $$n(w, j)$$ ëŠ” wë¥¼ ë£¨íŠ¸ë¡œ í•œ íŠ¸ë¦¬ì˜ j ë²ˆì§¸ ë…¸ë“œ
* $$L(w)$$ ëŠ” root ë¶€í„° w ê¹Œì§€ì˜ ê¸¸ì´
* $$ch(n)$$ ëŠ” n ì˜ child
* $$[\![x]\!]$$ ëŠ” x ê°€ True ë¼ë©´ 1 ì•„ë‹ˆë©´ -1
* *ê¸°ë³¸ì ìœ¼ë¡œ ìì‹ ë…¸ë“œëŠ” ì™¼ìª½ ìì‹ ë…¸ë“œë¥¼ ì˜ë¯¸*

ì´ ë¶€ë¶„ì„ ìƒê¸°í•˜ë©´ì„œ ì•„ë˜ ì‹ì„ ë³´ë„ë¡ í•˜ì.

$$
p\left(w \mid w_{I}\right)=\prod_{j=1}^{L(w)-1} \sigma\left([\![n(w, j+1)=\operatorname{ch}(n(w, j))]\!] \cdot v_{n(w, j)}^{\prime}{ }^{\top} v_{w_{I}}\right)
$$

Hierarchical Softmax Equation
{:.figcaption}

ìœ„ ê·¸ë¦¼ì„ ê°™ì´ ë³´ë©´ì„œ ì„¤ëª…í•˜ë©´ íŠ¸ë¦¬ëŠ” vocabulary í¬ê¸°ë§Œí¼ leaf ë…¸ë“œë¥¼ ê°€ì§„ë‹¤. ì € ì‹ì—ì„œ ì¤‘ìš”í•œ ê±´ ì•ˆì— ìˆëŠ” ì¼ì¢…ì˜ if ë¬¸ì´ë‹¤.

* j+1 ë²ˆì§¸ ë…¸ë“œê°€ jë²ˆì§¸ ë…¸ë“œì˜ ìì‹ì´ë¼ë©´     (ì™¼ìª½)   : `ì¤‘ì‹¬ ë‹¨ì–´ì™€ ê·¸ í•´ë‹¹ ì£¼ìœ„ ë‹¨ì–´ì˜ ë‚´ì ê°’`
* j+1 ë²ˆì§¸ ë…¸ë“œê°€ jë²ˆì§¸ ë…¸ë“œì˜ ìì‹ì´ ì•„ë‹ˆë¼ë©´ (ì˜¤ë¥¸ìª½)  : `-(ì¤‘ì‹¬ ë‹¨ì–´ì™€ ê·¸ í•´ë‹¹ ì£¼ìœ„ ë‹¨ì–´ì˜ ë‚´ì ê°’)`

í•´ë‹¹ í™•ë¥ ê°’ì„ maximize í•œë‹¤ëŠ” ê²ƒì„ ìŠì§€ ë§ì
{:.note}

ì´ë ‡ê²Œ êµ¬ì„±ì„ í•˜ê²Œ ë˜ë©´ `ì—°ì‚° íšŸìˆ˜ê°€ L(w)ì— ê·¼ì‚¬í•˜ê²Œ ë˜ê³  ì´ëŠ” íŠ¸ë¦¬ì˜ ë†’ì´`ì´ê¸°ì— $$log_2(V)$$ ê°€ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.
í•œë§ˆë””ë¡œ ê³„ì‚° íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™” í•œ ê²ƒì´ë‹¤. ë˜í•œ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì˜ ë…¸ë“œëŠ” ë£¨íŠ¸ ë…¸ë“œë¡œë¶€í„° ê±°ë¦¬ë¥¼ ê°€ê¹ê²Œ í•˜ì—¬ ë” ë¹ ë¥¸ ê³„ì‚°ì´ ê°€ëŠ¥í•˜ë„ë¡ í•˜ì˜€ë‹¤ê³  í•œë‹¤.
ë§ˆì§€ë§‰ ì¥ì ì€ í™•ë¥ ê°’ ê³„ì‚°ì— ì°¸ì—¬í•œ ë…¸ë“œë§Œ ì—…ë°ì´íŠ¸ ë˜ì–´ ì‹œê°„ì„ ì•„ë‚„ ìˆ˜ ìˆë‹¤.


## ğŸªš Negative Sampling

Hierarchical Softmax ì˜ ëŒ€ì•ˆìœ¼ë¡œ `Noise Contrastive Estimation(NCE)` ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì²˜ìŒ ë³´ëŠ” ìš©ì–´ì¸ë° ì´ê²ƒì´ ë¬´ì—‡ì¼ê¹Œ?

> * CBoW, Skip-gram ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë¹„ìš© ê³„ì‚° ì•Œê³ ë¦¬ì¦˜
> * ì „ì²´ ë°ì´í„°ì…‹ì— Softmax í•¨ìˆ˜ë¥¼ ì ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, ìƒ˜í”Œë§ìœ¼ë¡œ ì¶”ì¶œí•œ ì¼ë¶€ì— ëŒ€í•´ì„œë§Œ ì ìš©
> * NCE ë¥¼ ì‚¬ìš©í•  ê²½ìš° ë¬¸ì œë¥¼ ì‹¤ì œ context ì—ì„œ ì–»ì€ ë°ì´í„° ($$X$$) ì™€ context ì— ì†í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ë“¤ì—ì„œ ë½‘ì€ ë°ì´í„° ($$Y$$) ë¥¼ êµ¬ë³„í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¡œ ë°”ê¿€ ìˆ˜ ìˆìŒ
> * kê°œì˜ ëŒ€ë¹„ë˜ëŠ”(contrastive) ë‹¨ì–´ë“¤ì„ noise distributionì—ì„œ êµ¬í•´ì„œ (ëª¬í…Œì¹´ë¥¼ë¡œ) í‰ê· ì„ êµ¬í•˜ëŠ” ê²ƒì´ ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜
{:.lead}

NCE ëŠ” log í™•ë¥ ì„ maximize í•˜ëŠ”ë° ì´ˆì ì„ ë§ì¶”ê³  ìˆëŠ” ë°˜ë©´ Skip-gram ëª¨ë¸ì€ ì˜¤ì§ ê³ í’ˆì§ˆì˜ ë²¡í„°ë¥¼ í•™ìŠµí•˜ëŠ”ë° ì´ˆì ì„ ë§ì¶”ê³  ìˆë‹¤.
ê·¸ë ‡ê¸°ì— ë³¸ ë…¼ë¬¸ì€ ë²¡í„°ì˜ í’ˆì§ˆì€ ìœ ì§€í•˜ë©´ì„œ NCE ë¥¼ ê°„ì†Œí™”í•  ìˆ˜ ìˆì—ˆë‹¤ê³  í•œë‹¤. ì´ë ‡ê²Œ ê°„ì†Œí™”ëœ í˜•íƒœë¥¼ `Negative Sampling` ì´ë¼ í•œë‹¤.

$$
\log \sigma\left(v_{w_{O}}^{\prime}{ }^{\top} v_{w_{I}}\right)+\sum_{i=1}^{k} \mathbb{E}_{w_{i} \sim P_{n}(w)}\left[\log \sigma\left(-v_{w_{i}}^{\prime}{ }^{\top} v_{w_{I}}\right)\right]
$$

Negative Sampling
{:.figcaption}

ë…¼ë¬¸ì—ì„œëŠ” Skip-gram ì˜ objective í•¨ìˆ˜ì— ìˆëŠ” $$\log P\left(w_{O} \mid w_{I}\right)$$ ì‹ì„ ëª¨ë‘ ìœ„ ì‹ìœ¼ë¡œ êµì²´í•˜ì˜€ë‹¤ê³  í•œë‹¤. 

* ì¢Œì¸¡ term : ì…ë ¥ ë‹¨ì–´ $$w_I$$ ì— ëŒ€í•˜ì—¬ positive sample $$W_O$$ ê°€ output ì¼ í™•ë¥ ì„ Maximize
* ìš°ì¸¡ term : `Negative Sample` ì— ëŒ€í•˜ì—¬ $$W_I$$ ê°€ output ì´ ë  í™•ë¥ ì„ ìµœì†Œí™” â†’ ë‚´ì  ê²°ê³¼ì— -1 ì„ ê³±í•¨
  * Noise ë‹¨ì–´ë“¤ì˜ unigram í™•ë¥  ë¶„í¬ì¸ $$P_n(w)$$ ë¥¼ í†µí•´ sampling
  * Unigram Distributionì€ ë‹¨ì–´ê°€ ë“±ì¥í•˜ëŠ” ë¹„ìœ¨ì— ë¹„ë¡€í•˜ê²Œ í™•ë¥ ì„ ì„¤ì •í•˜ëŠ” ë¶„í¬
  * ë³¸ ë…¼ë¬¸ì—ì„œëŠ” unigram dist. ì— 3/4 ìŠ¹ í•œ ë¶„í¬($$U(w)^{3 / 4} / Z$$)ê°€ ì‹¤í—˜ì ìœ¼ë¡œ ê°€ì¥ ì¢‹ë‹¤ê³  í•¨

ê·¸ë ‡ë‹¤ë©´ NCE ì™€ NEG ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¼ê¹Œ?

* NCE : sample ê³¼ noise distributionì˜ í™•ë¥ ê°’ ëª¨ë‘ í•„ìš”
* NEG : sample ë§Œ í•„ìš”

NCE ê°™ì€ ê²½ìš° softmax ì˜ log í™•ë¥ ì„ maximize í•˜ëŠ” ê²ƒì´ ëª©í‘œì˜€ë‹¤. ì¦‰ ì˜ ë¶„ë¥˜í•˜ê³ ì í•˜ëŠ” ê²ƒì´ ëª©í‘œì˜€ìœ¼ë‚˜ í•´ë‹¹ ë…¼ë¬¸ì˜ ì£¼ì¶•ì¸ word2vec ê°™ì€ ê²½ìš°
`word representation` ì˜ í€„ë¦¬í‹°ë¥¼ ë†’ì´ëŠ” ê²ƒì„ ëª©í‘œë¡œ ì‚¼ì•˜ê¸°ì— Negative Sampling ì„ ì´ìš©í•œ ê²ƒì´ë‹¤.

## ğŸ‘ Subsampling of Frequent Words

ì—„ì²­ë‚œ ì–‘ì˜ ë°ì´í„°ë¥¼ í™œìš©í•˜ê²Œ ë˜ë©´ ì ì€ ì–‘ì˜ ì •ë³´ë¥¼ ì£¼ì§€ë§Œ ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ì´ ìˆë‹¤. (ex. "the", "a", "is", ...). í•˜ì§€ë§Œ í’ë¶€í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìˆëŠ” ë‹¨ì–´ë“¤ ì¤‘ì—ì„œëŠ”
ë¹ˆë„ìˆ˜ê°€ ë‚®ì€ ë‹¨ì–´ë“¤ë„ ìˆê¸° ë§ˆë ¨ì´ë‹¤.

ê° ë‹¨ì–´ë“¤ì˜ ë“±ì¥ íšŸìˆ˜ì˜ imbalance í•¨ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë³¸ ë…¼ë¬¸ì€ ê°„ë‹¨í•œ subsampling ê¸°ë²•ì„ ì ìš©í•œë‹¤. ê·¸ê²ƒì€ ë°”ë¡œ `discared probability`

* ì˜ë¯¸ì—†ëŠ” ë‹¤ë¹ˆë„ ë‹¨ì–´ë¥¼ ê±¸ëŸ¬ë‚´ê¸° ìœ„í•¨
* $$P\left(w_{i}\right)=1-\sqrt{\frac{t}{f\left(w_{i}\right)}}$$ ë¥¼ ì´ìš©í•˜ì—¬ í™•ë¥  ì„¤ì •
  * $$f(w_i)$$ ëŠ” ë‹¨ì–´ $$w_i$$ ê°€ ë“±ì¥í•˜ëŠ” ë¹ˆë„
  * $$P(w_i)$$ ëŠ” ë‹¨ì–´ $$w_i$$ ê°€ sampling ë˜ì§€ ì•Šì„ í™•ë¥ 
  * $$t$$ ëŠ” ì„¤ì •í•˜ëŠ” threshold

ì¦‰, ìì‹ ë“¤ì´ ì •í•œ threshold ë¥¼ ë„˜ê¸°ëŠ” ë¹ˆë„ìˆ˜ì˜ ë‹¨ì–´ë“¤ì„ sampling í•˜ê² ë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ í†µí•´ ë¹ˆë„ìˆ˜ê°€ ì ì§€ë§Œ ì¤‘ìš”í•œ ë‹¨ì–´ì˜ representation vector ì˜ í€„ë¦¬í‹°ë¥¼
í–¥ìƒí•˜ëŠ” ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆì—ˆë‹¤ê³  í•œë‹¤.

# ğŸï¸ Learning Phrases

ëŒ€ë¶€ë¶„ì˜ phrase ë“¤ì€ ë‹¨ìˆœíˆ ê°œë³„ ë‹¨ì–´ë“¤ì„ í•©ì¹œ ê²ƒì´ ì•„ë‹ˆë‹¤. ê·¸ë ‡ê¸°ì— ê·¸ phrase ì˜ representation vector ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´
íŠ¹ì • phrase ì—ì„œë§Œ ë¹ˆë„ ìˆ˜ê°€ ë†’ì€ ë‹¨ì–´ ìŒì„ ì°¾ëŠ” ê²ƒì„ ì‹œì‘ìœ¼ë¡œ í–ˆë‹¤ê³  í•œë‹¤.

ì˜ˆë¥¼ ë“¤ë©´, `New York Times, Toronto Maple Leafs` ì™€ ê°™ì´ ê³ ìœ í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ ê²ƒë“¤ì€ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ì¹˜í™˜í•˜ì˜€ë‹¤ê³  í•˜ê³  `this is, there are` ê°™ì€
ì˜ë¯¸ ì—†ì´ ë§ì´ ë‚˜ì˜¤ëŠ” ê²ƒë“¤ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì˜€ë‹¤ê³  í•œë‹¤.

ì´ëŸ¬í•œ ë°©ì‹ì„ í†µí•´ vocabulary ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë¦¬ì§€ ì•Šê³  phrase ë¥¼ ì§ì ‘ ì§€ì •í•´ì£¼ì—ˆê³ , data-driven ì„ í†µí•´ ìë™ìœ¼ë¡œ ì°¾ì„ ìˆ˜ ìˆë„ë¡ í•˜ì˜€ë‹¤ê³  í•œë‹¤.
ê·¸ë ‡ê²Œ í•˜ê¸° ìœ„í•´ ì•„ë˜ì˜ ì‹ì„ ì ìš©í•´ ì •í•´ë†“ì€ ê¸°ì¤€ë³´ë‹¤ ë†’ìœ¼ë©´ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ ì¸ì‹í•˜ë„ë¡ í•˜ì˜€ë‹¤ê³  í•œë‹¤.

$$
\operatorname{score}\left(w_{i}, w_{j}\right)=\frac{\operatorname{count}\left(w_{i} w_{j}\right)-\delta}{\operatorname{count}\left(w_{i}\right) \times \operatorname{count}\left(w_{j}\right)} .
$$

Phrase Score
{:.figcaption}

> score = ë‹¨ì–´ê°€ ë™ì‹œì— ë“±ì¥í•˜ëŠ” íšŸìˆ˜ - $$\delta$$ / ê° ë‹¨ì–´ê°€ ë“±ì¥í•˜ëŠ” íšŸìˆ˜ì˜ ê³±
> 
> ì—¬ê¸°ì„œ $$\delta$$ ëŠ” ë„ˆë¬´ ë“œë¬¼ê²Œ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ì˜ ì¡°í•©ì´ í•˜ë‚˜ì˜ êµ¬ë¡œ ë§Œë“¤ì–´ì§€ì§€ ì•Šê¸° ìœ„í•œ hyper parameter
{:.lead}

![image](https://github.com/jungsiroo/jungsiroo.github.io/assets/54366260/1d7f2a74-2e98-44cf-af09-134084d65a53)

analogy test dataset
{:.figcaption}

ì´ 5ê°œì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ `analogy test` ë¥¼ ìˆ˜í–‰í•˜ì˜€ë‹¤ê³  í•œë‹¤. ê°ê° ì•ìª½ 3ê°œì˜ ì—´ì„ ì´ìš©í•´ ë§ˆì§€ë§‰ ì—´ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œì´ë‹¤.

# âš™ï¸ Additive Compositionality

ë§ˆì§€ë§‰ìœ¼ë¡œ ë³¼ ê²ƒì€ ì•ì„œ ì–¸ê¸‰í–ˆë˜ í•©ì„±ì„±ì´ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Skip-gram ëª¨ë¸ì„ í†µí•´ í•™ìŠµëœ `word and phrase representations` ë“¤ì„ í†µí•´ 
ê°„ë‹¨í•œ ì—°ì‚°ì„ í†µí•´ analogical reasoning íƒœìŠ¤í¬ë¥¼ ì •í™•í•˜ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.

> * Skip-gram ì˜ ëª©ì ì€ ê³ í’ˆì§ˆì˜ word representation ë“¤ì„ í•™ìŠµí•˜ëŠ” ê²ƒ
>   * ì´ëŠ” ì¤‘ì‹¬ë¶€ ë‹¨ì–´ì˜ context ë¥¼ ì´ìš©í•´ ì£¼ë³€ ë‹¨ì–´ë¥¼ ë§ì¶œ í™•ë¥ ì„ maximize í•˜ëŠ” ê²ƒ
> * ë‘ ë‹¨ì–´ì˜ vector ë¥¼ ë”í•œë‹¤ëŠ” ê²ƒì€ ë‘ ë¬¸ë§¥ì„ AND ì—°ì‚°í•œë‹¤ëŠ” ê²ƒ
> * ê·¸ë ‡ê¸°ì— ë‘ ë‹¨ì–´ì˜ í•©ì„ í†µí•´ ê·¸ ë‹¨ì–´ê°€ í¬í•¨ë˜ìˆë˜ context ì •ë³´ë¥¼ í•©ì¹  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ
{:.lead}

![image](https://github.com/jungsiroo/jungsiroo.github.io/assets/54366260/c9419b4f-1fdd-46dc-89fe-464b405c08a8)

element-wise addition
{:.figcaption}

# ğŸš€ Conclusion

ë…¼ë¬¸ì„ ëë§ˆì¹˜ë©´ì„œ íŠ¹ì´í•˜ê²Œ representation vector ì˜ í’ˆì§ˆì— ì˜í–¥ì„ ì¤¬ë˜ hyper parameter ë“¤ì„ ì†Œê°œí•œë‹¤.

* Choice of Model Architecture
* Size of the vectors
* Subsampling rate
* Size of the training window.

í•˜ì§€ë§Œ ì—¬ì „íˆ OOV ë¬¸ì œëŠ” í•´ê²°í•˜ì§€ ëª»í•˜ì˜€ê³  (2013ë…„ì´ì˜€ê¸°ì—...) Additive Compositionality ì—ì„œ ë‹¨ìˆœ ë”í•˜ê¸°ë§Œ í•  ìˆ˜ ìˆëŠ”ê±´ì§€ ì˜ë¬¸ì´ ë“ ë‹¤.
ë˜í•œ subsmapling of frequent words ë¥¼ í†µí•´ ì–´ëŠ ì •ë„ì˜ less frequent words ì˜ accuracy ë¥¼ ì–¼ë§ˆë‚˜ ë†’ì˜€ëŠ”ì§€ë¥¼ ì•Œ ìˆ˜ ì—†ì–´ ì•„ì‰¬ì›€ì´ ë‚¨ëŠ”ë‹¤.
ì´ì „ ë…¼ë¬¸ì—ì„œëŠ” less frequent words ëŠ” poor representation vector ê°€ í•™ìŠµë˜ì—ˆê³  ê°œì„  ê²°ê³¼ë¥¼ ìˆ˜ì¹˜ë¡œ í‘œí˜„í–ˆìœ¼ë©´ ì–´ë–¨ê¹Œí•˜ëŠ” ìƒê°ì´ ë“ ë‹¤.